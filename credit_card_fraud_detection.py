"""Credit Card Fraud Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aifN65kPwOb0_4lsu8r9WiUu3mmNOmJS

# Credit Card Fraud Detection

This code implements a machine learning pipeline for detecting fraudulent credit card transactions.
It handles class imbalance, performs feature engineering, and evaluates multiple models.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek
from imblearn.pipeline import Pipeline
import joblib
import warnings

"""## 1. Data Loading and Exploration"""

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

print(f"Dataset shape: {df.shape}")
print(f"\nData types:\n{df.dtypes}")
print(f"\nChecking for missing values:\n{df.isnull().sum().sum()} missing values")
print(f"\nClass distribution:")
fraud_count = df['Class'].value_counts()
print(fraud_count)
print(f"Fraud percentage: {fraud_count[1]/len(df)*100:.4f}%")

import os
if not os.path.exists('visualizations'):
    os.makedirs('visualizations')

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='Class', data=df)
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom')
plt.title('Class Distribution (Fraud vs Normal)')
plt.yscale('log')
plt.show()
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['Amount'], kde=True)
plt.title('Distribution of Transaction Amount')
plt.xlabel('Amount')

plt.subplot(1, 2, 2)
sns.histplot(df[df['Class']==0]['Amount'], kde=True, color='blue', alpha=0.5, label='Normal')
sns.histplot(df[df['Class']==1]['Amount'], kde=True, color='red', alpha=0.5, label='Fraud')
plt.title('Amount Distribution by Class')
plt.xlabel('Amount')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['Time'], kde=True)
plt.title('Distribution of Transaction Time')
plt.xlabel('Time (seconds)')

plt.subplot(1, 2, 2)
if 'Hour' not in df.columns:
    df['Hour'] = (df['Time'] / 3600) % 24
sns.histplot(df['Hour'], bins=24, kde=True)
plt.title('Transaction Distribution by Hour of Day')
plt.xlabel('Hour of Day')
plt.xticks(range(0, 24, 2))
plt.tight_layout()
plt.show()

"""## 2. Data Preprocessing"""

print(f"Number of NaN values per column:\n{df.isna().sum()}")

print(f"\nNaN values in Class column: {df['Class'].isna().sum()}")

df_clean = df.dropna(subset=['Class'])
print(f"\nDataset shape after dropping NaN Class values: {df_clean.shape}")

X = df_clean.drop('Class', axis=1)
y = df_clean['Class']

print(f"NaN values in y after cleaning: {y.isna().sum()}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\nTraining set shape: {X_train.shape}")
print(f"Training set class distribution: {pd.Series(y_train).value_counts()}")
print(f"Testing set shape: {X_test.shape}")
print(f"Testing set class distribution: {pd.Series(y_test).value_counts()}")

"""## 3. Feature Engineering"""

X_train['Hour'] = (X_train['Time'] / 3600) % 24
X_test['Hour'] = (X_test['Time'] / 3600) % 24

X_train['Day'] = ((X_train['Time'] / 3600) / 24).astype(int) % 7
X_test['Day'] = ((X_test['Time'] / 3600) / 24).astype(int) % 7

X_train['IsWeekend'] = X_train['Day'].apply(lambda x: 1 if x >= 5 else 0)
X_test['IsWeekend'] = X_test['Day'].apply(lambda x: 1 if x >= 5 else 0)

print("Creating amount-related features...")

X_train['AmountBin'] = pd.qcut(X_train['Amount'], q=10, labels=False, duplicates='drop')
X_test['AmountBin'] = pd.qcut(X_test['Amount'], q=10, labels=False, duplicates='drop')

X_train['LogAmount'] = np.log1p(X_train['Amount'])
X_test['LogAmount'] = np.log1p(X_test['Amount'])

amount_mean = X_train.groupby('Hour')['Amount'].transform('mean')
amount_median = X_train.groupby('Hour')['Amount'].transform('median')
amount_max = X_train.groupby('Hour')['Amount'].transform('max')
amount_std = X_train.groupby('Hour')['Amount'].transform('std')

X_train['AmountDiffFromHourlyMean'] = X_train['Amount'] - amount_mean
X_train['AmountDiffFromHourlyMedian'] = X_train['Amount'] - amount_median
X_train['AmountToHourlyMaxRatio'] = X_train['Amount'] / (amount_max + 1)
X_train['AmountZScoreHourly'] = X_train['AmountDiffFromHourlyMean'] / (amount_std + 1)
hour_mean_map = X_train.groupby('Hour')['Amount'].mean().to_dict()
hour_median_map = X_train.groupby('Hour')['Amount'].median().to_dict()
hour_max_map = X_train.groupby('Hour')['Amount'].max().to_dict()
hour_std_map = X_train.groupby('Hour')['Amount'].std().to_dict()

X_test['AmountDiffFromHourlyMean'] = X_test.apply(lambda x: x['Amount'] - hour_mean_map.get(x['Hour'], 0), axis=1)
X_test['AmountDiffFromHourlyMedian'] = X_test.apply(lambda x: x['Amount'] - hour_median_map.get(x['Hour'], 0), axis=1)
X_test['AmountToHourlyMaxRatio'] = X_test.apply(lambda x: x['Amount'] / (hour_max_map.get(x['Hour'], 1) + 1), axis=1)
X_test['AmountZScoreHourly'] = X_test.apply(lambda x: x['AmountDiffFromHourlyMean'] / (hour_std_map.get(x['Hour'], 1) + 1), axis=1)

amount_day_mean = X_train.groupby('Day')['Amount'].transform('mean')
amount_day_std = X_train.groupby('Day')['Amount'].transform('std')

X_train['AmountZScoreDaily'] = (X_train['Amount'] - amount_day_mean) / (amount_day_std + 1)

day_mean_map = X_train.groupby('Day')['Amount'].mean().to_dict()
day_std_map = X_train.groupby('Day')['Amount'].std().to_dict()

X_test['AmountZScoreDaily'] = X_test.apply(lambda x: (x['Amount'] - day_mean_map.get(x['Day'], 0)) / (day_std_map.get(x['Day'], 1) + 1), axis=1)

X_train['V1_V2'] = X_train['V1'] * X_train['V2']
X_train['V1_V3'] = X_train['V1'] * X_train['V3']
X_train['V2_V3'] = X_train['V2'] * X_train['V3']

X_test['V1_V2'] = X_test['V1'] * X_test['V2']
X_test['V1_V3'] = X_test['V1'] * X_test['V3']
X_test['V2_V3'] = X_test['V2'] * X_test['V3']


X_train['ProfileBin'] = pd.qcut(X_train['V1'] + X_train['V2'], q=100, labels=False, duplicates='drop')
X_test['ProfileBin'] = pd.qcut(X_test['V1'] + X_test['V2'], q=100, labels=False, duplicates='drop')

profile_hour_count = X_train.groupby(['ProfileBin', 'Hour']).size().reset_index(name='TransactionFrequency')
profile_hour_map = profile_hour_count.set_index(['ProfileBin', 'Hour'])['TransactionFrequency'].to_dict()

X_train['TxCountByProfileAndHour'] = X_train.apply(
    lambda x: profile_hour_map.get((x['ProfileBin'], x['Hour']), 0), axis=1
)

X_test['TxCountByProfileAndHour'] = X_test.apply(
    lambda x: profile_hour_map.get((x['ProfileBin'], x['Hour']), 0)
    if (x['ProfileBin'], x['Hour']) in profile_hour_map else 0,
    axis=1
)

profile_day_count = X_train.groupby(['ProfileBin', 'Day']).size().reset_index(name='DailyFrequency')
profile_day_map = profile_day_count.set_index(['ProfileBin', 'Day'])['DailyFrequency'].to_dict()

X_train['TxCountByProfileAndDay'] = X_train.apply(
    lambda x: profile_day_map.get((x['ProfileBin'], x['Day']), 0), axis=1
)

X_test['TxCountByProfileAndDay'] = X_test.apply(
    lambda x: profile_day_map.get((x['ProfileBin'], x['Day']), 0)
    if (x['ProfileBin'], x['Day']) in profile_day_map else 0,
    axis=1
)

print("Feature engineering complete. Final feature set:")
print(f"Training features: {X_train.shape[1]} columns")
print("Sample of new features:")
print(X_train[['Hour', 'Day', 'IsWeekend', 'AmountBin', 'LogAmount',
              'AmountDiffFromHourlyMean', 'TxCountByProfileAndHour']].head())

scaler = StandardScaler()
cat_cols = ['Hour', 'Day', 'IsWeekend', 'AmountBin', 'ProfileBin']
num_cols = [col for col in X_train.columns if col not in cat_cols]

X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])

print(f"Scaled {len(num_cols)} numerical features")

"""## 4. Model Selection and Evaluation"""

def evaluate_model(y_true, y_pred, y_prob, model_name):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_prob)
    pr_auc = auc(recall_curve, precision_curve)

    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)

    print(f"\n{model_name} Evaluation:")
    print(f"Confusion Matrix:\n{cm}")
    print(f"Precision (Positive Predictive Value): {precision:.4f}")
    print(f"Recall (Sensitivity): {recall:.4f}")
    print(f"Specificity: {specificity:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"PR AUC: {pr_auc:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")

    # Create visualizations
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(recall_curve, precision_curve, label=f'AUC = {pr_auc:.4f}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'{model_name} Precision-Recall Curve')
    plt.legend()

    # ROC curve
    plt.subplot(1, 2, 2)
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{model_name} ROC Curve')
    plt.legend()

    plt.tight_layout()
    plt.show()

    return {
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'f1': f1,
        'pr_auc': pr_auc,
        'roc_auc': roc_auc
    }

"""## 5. Handling Class Imbalance and Training Models"""

# Check for NaN values in the original dataset
print(f"Number of NaN values per column in original dataset:\n{df.isna().sum()}")
print(f"Total NaN values in original dataset: {df.isna().sum().sum()}")

# Clean the dataset by dropping rows with NaN in any column (or just the Class column if preferred)
df_clean = df.dropna()
print(f"\nDataset shape after dropping rows with NaN values: {df_clean.shape}")

# Separate features and target from the cleaned dataset
X = df_clean.drop('Class', axis=1)
y = df_clean['Class']

# Split the data into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\nTraining set shape: {X_train.shape}")
print(f"Training set class distribution: {pd.Series(y_train).value_counts()}")
print(f"Testing set shape: {X_test.shape}")
print(f"Testing set class distribution: {pd.Series(y_test).value_counts()}")

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Verify no NaN values in scaled data
print(f"\nNaN values in X_train_scaled: {np.isnan(X_train_scaled).sum()}")
print(f"NaN values in X_test_scaled: {np.isnan(X_test_scaled).sum()}")

model_results = {}

# 5.1 SMOTE Oversampling with XGBoost
print("\nTraining XGBoost with SMOTE...")

smote_pipeline = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))
])

smote_pipeline.fit(X_train_scaled, y_train)
y_pred_smote = smote_pipeline.predict(X_test_scaled)
y_prob_smote = smote_pipeline.predict_proba(X_test_scaled)[:, 1]

# Assuming your evaluate_model function exists
smote_results = evaluate_model(y_test, y_pred_smote, y_prob_smote, "XGBoost_SMOTE")
model_results["XGBoost_SMOTE"] = smote_results

# 5.2 Random Undersampling with Random Forest
print("\nTraining Random Forest with Random Undersampling...")

rus_pipeline = Pipeline([
    ('undersampler', RandomUnderSampler(random_state=42)),
    ('classifier', RandomForestClassifier(random_state=42))
])

rus_pipeline.fit(X_train_scaled, y_train)
y_pred_rus = rus_pipeline.predict(X_test_scaled)
y_prob_rus = rus_pipeline.predict_proba(X_test_scaled)[:, 1]

rus_results = evaluate_model(y_test, y_pred_rus, y_prob_rus, "RandomForest_RUS")
model_results["RandomForest_RUS"] = rus_results

# 5.3 SMOTETomek Combined Approach with Gradient Boosting
print("\nTraining Gradient Boosting with SMOTETomek...")

smote_tomek_pipeline = Pipeline([
    ('smote_tomek', SMOTETomek(random_state=42)),
    ('classifier', GradientBoostingClassifier(random_state=42))
])

smote_tomek_pipeline.fit(X_train_scaled, y_train)
y_pred_smote_tomek = smote_tomek_pipeline.predict(X_test_scaled)
y_prob_smote_tomek = smote_tomek_pipeline.predict_proba(X_test_scaled)[:, 1]

smote_tomek_results = evaluate_model(y_test, y_pred_smote_tomek, y_prob_smote_tomek, "GradientBoosting_SMOTETomek")
model_results["GradientBoosting_SMOTETomek"] = smote_tomek_results

# 5.4 Cost-sensitive Learning (without resampling)
print("\nTraining Cost-sensitive Logistic Regression...")

# Class weights to handle imbalance (inverse of class frequency)
class_weight = {0: 1, 1: len(y_train)/sum(y_train)}
lr_model = LogisticRegression(class_weight=class_weight, random_state=42, max_iter=1000, solver='liblinear')
lr_model.fit(X_train_scaled, y_train)
y_pred_lr = lr_model.predict(X_test_scaled)
y_prob_lr = lr_model.predict_proba(X_test_scaled)[:, 1]

lr_results = evaluate_model(y_test, y_pred_lr, y_prob_lr, "CostSensitive_LogisticRegression")
model_results["CostSensitive_LogisticRegression"] = lr_results

# 5.5 Ensemble Model (Voting Classifier)
print("\nTraining Ensemble Model (Voting Classifier)...")

# Create base models
rf = RandomForestClassifier(n_estimators=100, random_state=42)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
xgb = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')

# Create the ensemble
ensemble = VotingClassifier(
    estimators=[('rf', rf), ('gb', gb), ('xgb', xgb)],
    voting='soft'
)

# Train on balanced dataset with SMOTE
X_train_balanced, y_train_balanced = SMOTE(random_state=42).fit_resample(X_train_scaled, y_train)
ensemble.fit(X_train_balanced, y_train_balanced)

y_pred_ensemble = ensemble.predict(X_test_scaled)
y_prob_ensemble = ensemble.predict_proba(X_test_scaled)[:, 1]

ensemble_results = evaluate_model(y_test, y_pred_ensemble, y_prob_ensemble, "VotingEnsemble_SMOTE")
model_results["VotingEnsemble_SMOTE"] = ensemble_results

"""## 6. Comparing Models"""

models_pr_auc = {model: results['pr_auc'] for model, results in model_results.items()}
best_model = max(models_pr_auc, key=models_pr_auc.get)

print("\nModel Comparison (PR AUC):")
for model, pr_auc in models_pr_auc.items():
    print(f"{model}: {pr_auc:.4f}")
print(f"\nBest Model: {best_model} with PR AUC = {models_pr_auc[best_model]:.4f}")

plt.figure(figsize=(10, 6))
plt.bar(models_pr_auc.keys(), models_pr_auc.values())
plt.xlabel('Model')
plt.ylabel('PR AUC Score')
plt.title('Model Performance Comparison (PR AUC)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('visualizations/model_comparison.png')
plt.close()

"""## 7. Hyperparameter Tuning for the Best Model"""

if best_model == "XGBoost_SMOTE":
    print("\nTuning XGBoost with SMOTE...")
    param_grid = {
        'classifier__learning_rate': [0.01, 0.1],
        'classifier__max_depth': [3, 5, 7],
        'classifier__n_estimators': [100, 200],
        'classifier__subsample': [0.8, 1.0],
        'classifier__colsample_bytree': [0.8, 1.0]
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(estimator=smote_pipeline, param_grid=param_grid, cv=cv,
                             scoring='average_precision', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)

elif best_model == "RandomForest_RUS":
    print("\nTuning Random Forest with Undersampling...")
    param_grid = {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [10, 15, None],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [1, 2, 4]
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(estimator=rus_pipeline, param_grid=param_grid, cv=cv,
                              scoring='average_precision', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)

elif best_model == "GradientBoosting_SMOTETomek":
    print("\nTuning Gradient Boosting with SMOTETomek...")
    param_grid = {
        'classifier__learning_rate': [0.01, 0.1],
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [3, 5, 7],
        'classifier__subsample': [0.8, 1.0]
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(estimator=smote_tomek_pipeline, param_grid=param_grid, cv=cv,
                              scoring='average_precision', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)

elif best_model == "VotingEnsemble_SMOTE":
    print("\nTuning individual components of the Voting Ensemble...")

    xgb_tuned = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
    param_grid = {
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5, 7],
        'n_estimators': [100, 200],
        'subsample': [0.8, 1.0]
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, cv=cv,
                              scoring='average_precision', n_jobs=-1)
    grid_search.fit(X_train_balanced, y_train_balanced)

    ensemble = VotingClassifier(
        estimators=[('rf', rf), ('gb', gb), ('xgb', grid_search.best_estimator_)],
        voting='soft'
    )
    ensemble.fit(X_train_balanced, y_train_balanced)
    grid_search = ensemble

else:
    print("\nTuning Logistic Regression...")
    param_grid = {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear']
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(estimator=lr_model, param_grid=param_grid, cv=cv,
                              scoring='average_precision', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)

print(f"Best parameters: {grid_search.best_params_ if hasattr(grid_search, 'best_params_') else 'N/A'}")

y_pred_tuned = grid_search.predict(X_test_scaled)
y_prob_tuned = grid_search.predict_proba(X_test_scaled)[:, 1]
tuned_results = evaluate_model(y_test, y_pred_tuned, y_prob_tuned, f"{best_model}_Tuned")

"""## 8. Finding Optimal Threshold"""

thresholds = np.arange(0.05, 0.96, 0.05)
results = []

for threshold in thresholds:
    y_pred_thresh = (y_prob_tuned >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

    results.append({
        'threshold': threshold,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'specificity': specificity,
        'fp_rate': fp / (fp + tn) if (fp + tn) > 0 else 0
    })

results_df = pd.DataFrame(results)

# Plotting threshold analysis
plt.figure(figsize=(14, 10))

plt.subplot(2, 1, 1)
plt.plot(results_df['threshold'], results_df['precision'], label='Precision', marker='o')
plt.plot(results_df['threshold'], results_df['recall'], label='Recall', marker='s')
plt.plot(results_df['threshold'], results_df['f1'], label='F1 Score', marker='^')
plt.plot(results_df['threshold'], results_df['specificity'], label='Specificity', marker='*')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision, Recall, F1, and Specificity vs. Threshold')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(results_df['threshold'], results_df['fp_rate'], color='red', marker='o')
plt.xlabel('Threshold')
plt.ylabel('False Positive Rate')
plt.title('False Positive Rate vs. Threshold')
plt.grid(True)

plt.tight_layout()
plt.savefig('visualizations/threshold_analysis.png')
plt.close()

optimal_threshold_f1 = results_df.loc[results_df['f1'].idxmax(), 'threshold']
print(f"Optimal threshold based on F1 score: {optimal_threshold_f1}")

precision_recall_diff = abs(results_df['precision'] - results_df['recall'])
balanced_threshold = results_df.loc[precision_recall_diff.idxmin(), 'threshold']
print(f"Balanced threshold (precision â‰ˆ recall): {balanced_threshold}")

min_precision = 0.9
precision_thresholds = results_df[results_df['precision'] >= min_precision]
if not precision_thresholds.empty:
    business_threshold = precision_thresholds.iloc[0]['threshold']
    print(f"Business threshold (precision >= {min_precision}): {business_threshold}")
else:
    business_threshold = 0.95
    print(f"No threshold with precision >= {min_precision}, using default: {business_threshold}")

final_preds = (y_prob_tuned >= optimal_threshold_f1).astype(int)
final_results = evaluate_model(y_test, final_preds, y_prob_tuned, "Final_Model_F1_Optimal_Threshold")

business_preds = (y_prob_tuned >= business_threshold).astype(int)
business_results = evaluate_model(y_test, business_preds, y_prob_tuned, "Final_Model_Business_Threshold")

"""## 9. FEATURE IMPORTANCE ANALYSIS"""

feature_names = X_train.columns if hasattr(X_train, 'columns') else np.array([f'feature_{i}' for i in range(X_train.shape[1])])

if hasattr(grid_search, 'best_estimator_'):
    best_model_obj = grid_search.best_estimator_

    if hasattr(best_model_obj, 'named_steps'):
        if 'classifier' in best_model_obj.named_steps:
            classifier = best_model_obj.named_steps['classifier']
            if hasattr(classifier, 'feature_importances_'):
                feature_importance = classifier.feature_importances_
                features = feature_names
            elif hasattr(classifier, 'coef_'):
                feature_importance = np.abs(classifier.coef_[0])
                features = feature_names
            else:
                print("Cannot extract feature importance from this model type")
                feature_importance = np.ones(X_train_scaled.shape[1])
                features = feature_names
        else:
            print("Pipeline doesn't have a 'classifier' step")
            feature_importance = np.ones(X_train_scaled.shape[1])
            features = feature_names
    elif hasattr(best_model_obj, 'estimators_'):
        importance_found = False
        for name, estimator in best_model_obj.named_estimators_.items():
            if name == 'xgb' and hasattr(estimator, 'feature_importances_'):
                feature_importance = estimator.feature_importances_
                features = feature_names
                importance_found = True
                break
        if not importance_found:
            print("Cannot extract feature importance from ensemble")
            feature_importance = np.ones(X_train_scaled.shape[1])
            features = feature_names
    else:
        if hasattr(best_model_obj, 'feature_importances_'):
            feature_importance = best_model_obj.feature_importances_
            features = feature_names
        elif hasattr(best_model_obj, 'coef_'):
            feature_importance = np.abs(best_model_obj.coef_[0])
            features = feature_names
        else:
            print("Cannot extract feature importance from this model type")
            feature_importance = np.ones(X_train_scaled.shape[1])
            features = feature_names
else:
    print("No best_estimator_ attribute found, using placeholder importance values")
    feature_importance = np.ones(X_train_scaled.shape[1])
    features = feature_names

feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importance
})
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

top_features = feature_importance_df.head(20)

# Create bar chart of feature importance
plt.figure(figsize=(14, 10))
sns.barplot(x='Importance', y='Feature', data=top_features)
plt.title('Top 20 Feature Importance')
plt.tight_layout()
plt.savefig('visualizations/feature_importance.png')
plt.show()
plt.close()

print("\nTop 10 most important features:")
print(feature_importance_df.head(10))

"""## 10. MODEL PERSISTENCE"""

model_filename = f'final_model_{best_model}.pkl'
joblib.dump(grid_search, model_filename)
print(f"Final model saved as {model_filename}")

joblib.dump(scaler, 'scaler.pkl')
print("Feature scaler saved as scaler.pkl")

feature_metadata = {
    'categorical_columns': cat_cols,
    'numerical_columns': num_cols,
    'hour_mean_map': hour_mean_map,
    'hour_median_map': hour_median_map,
    'hour_max_map': hour_max_map,
    'hour_std_map': hour_std_map,
    'day_mean_map': day_mean_map,
    'day_std_map': day_std_map,
    'optimal_threshold': optimal_threshold_f1,
    'business_threshold': business_threshold
}
joblib.dump(feature_metadata, 'feature_metadata.pkl')
print("Feature metadata saved as feature_metadata.pkl")

"""## 11. INFERENCE PIPELINE"""

def preprocess_transaction(transaction_data, scaler, feature_metadata):

    X = transaction_data.copy()

    X['Hour'] = (X['Time'] / 3600) % 24
    X['Day'] = ((X['Time'] / 3600) / 24).astype(int) % 7
    X['IsWeekend'] = X['Day'].apply(lambda x: 1 if x >= 5 else 0)
    X['AmountBin'] = pd.cut(X['Amount'], bins=10, labels=False, duplicates='drop')
    X['LogAmount'] = np.log1p(X['Amount'])

    if all(k in feature_metadata for k in ['hour_mean_map', 'hour_median_map', 'hour_max_map', 'hour_std_map']):
        X['AmountDiffFromHourlyMean'] = X.apply(
            lambda x: x['Amount'] - feature_metadata['hour_mean_map'].get(x['Hour'], 0), axis=1
        )
        X['AmountDiffFromHourlyMedian'] = X.apply(
            lambda x: x['Amount'] - feature_metadata['hour_median_map'].get(x['Hour'], 0), axis=1
        )
        X['AmountToHourlyMaxRatio'] = X.apply(
            lambda x: x['Amount'] / (feature_metadata['hour_max_map'].get(x['Hour'], 1) + 1), axis=1
        )
        X['AmountZScoreHourly'] = X.apply(
            lambda x: (x['Amount'] - feature_metadata['hour_mean_map'].get(x['Hour'], 0)) /
                     (feature_metadata['hour_std_map'].get(x['Hour'], 1) + 1), axis=1
        )

    if all(k in feature_metadata for k in ['day_mean_map', 'day_std_map']):
        X['AmountZScoreDaily'] = X.apply(
            lambda x: (x['Amount'] - feature_metadata['day_mean_map'].get(x['Day'], 0)) /
                     (feature_metadata['day_std_map'].get(x['Day'], 1) + 1), axis=1
        )

    X['V1_V2'] = X['V1'] * X['V2']
    X['V1_V3'] = X['V1'] * X['V3']
    X['V2_V3'] = X['V2'] * X['V3']

    X['ProfileBin'] = pd.cut(X['V1'] + X['V2'], bins=100, labels=False, duplicates='drop')
    X['TxCountByProfileAndHour'] = 1
    X['TxCountByProfileAndDay'] = 1

    if 'model_features' in feature_metadata:
        model_features = feature_metadata['model_features']
    else:
        if 'X_train' in globals():
            model_features = X_train.columns.tolist()
        else:
            model_features = [col for col in X.columns if col.startswith('V')]

    for col in model_features:
        if col not in X.columns:
            X[col] = 0

    X = X[model_features]

    # Scale the features
    if scaler is not None:
        X = pd.DataFrame(scaler.transform(X), columns=X.columns)

    return X

def predict_fraud(transaction_data, model, scaler, feature_metadata, threshold=0.5):

    if 'optimal_threshold' in feature_metadata:
        threshold = feature_metadata['optimal_threshold']

    X_processed = preprocess_transaction(transaction_data, scaler, feature_metadata)

    if hasattr(model, 'predict_proba'):
        fraud_probability = model.predict_proba(X_processed)[:, 1]
        is_fraud = (fraud_probability >= threshold).astype(int)
    else:

        is_fraud = model.predict(X_processed)
        fraud_probability = is_fraud.astype(float)

    results = []
    for i, (prob, fraud) in enumerate(zip(fraud_probability, is_fraud)):
        results.append({
            'transaction_index': i,
            'fraud_probability': prob,
            'is_fraud': bool(fraud),
            'threshold_used': threshold
        })
    return results

feature_metadata = {
    'hour_mean_map': {h: 0 for h in range(24)},
    'hour_median_map': {h: 0 for h in range(24)},
    'hour_max_map': {h: 1 for h in range(24)},
    'hour_std_map': {h: 1 for h in range(24)},
    'day_mean_map': {d: 0 for d in range(7)},
    'day_std_map': {d: 1 for d in range(7)},
    'optimal_threshold': 0.5
}

feature_metadata['model_features'] = X_train.columns.tolist()

loaded_model = grid_search
loaded_scaler = scaler
loaded_metadata = feature_metadata

sample_transactions = X_test.head(5).copy()
print(f"\nSample transactions shape: {sample_transactions.shape}")

try:
    predictions = predict_fraud(sample_transactions, loaded_model, loaded_scaler, loaded_metadata)

    print("\nPrediction results:")
    for pred in predictions:
        print(f"Transaction {pred['transaction_index']}: "
              f"Fraud Probability = {pred['fraud_probability']:.4f}, "
              f"Is Fraud = {pred['is_fraud']}")

except Exception as e:
    print(f"\nError in prediction: {str(e)}")
    print("\nFallback to direct model prediction:")

    if hasattr(loaded_model, 'predict_proba'):
        probs = loaded_model.predict_proba(sample_transactions)[:, 1]
        preds = (probs >= 0.5).astype(int)

        for i, (prob, is_fraud) in enumerate(zip(probs, preds)):
            print(f"Transaction {i}: Fraud Probability = {prob:.4f}, Is Fraud = {bool(is_fraud)}")
    else:
        preds = loaded_model.predict(sample_transactions)
        for i, is_fraud in enumerate(preds):
            print(f"Transaction {i}: Is Fraud = {bool(is_fraud)}")

"""## 12. SUMMARY"""

if hasattr(X_train_scaled, 'columns'):
    num_features_after = len(X_train_scaled.columns)
else:

    num_features_after = X_train_scaled.shape[1]

# Original feature count (minus target)
original_features = len(df.columns) - 1
# New features created
additional_features = num_features_after - original_features

print(f"""
Credit Card Fraud Detection Model Summary:
1. Dataset Information:
 - Total transactions analyzed: {len(df)}
 - Fraud transactions: {fraud_count[1]} ({fraud_count[1]/len(df)*100:.4f}%)
 - Normal transactions: {fraud_count[0]} ({fraud_count[0]/len(df)*100:.4f}%)
2. Feature Engineering:
 - Original features: {original_features}
 - Additional features created: {additional_features}
 - Total features used: {num_features_after}
3. Best Model:
 - Model type: {best_model}
 - PR AUC score: {models_pr_auc[best_model]:.4f}
 - After tuning PR AUC: {tuned_results['pr_auc']:.4f}
4. Optimal Threshold Results:
 - F1-optimal threshold: {optimal_threshold_f1:.3f}
 - Precision: {final_results['precision']:.4f}
 - Recall: {final_results['recall']:.4f}
 - F1 score: {final_results['f1']:.4f}
 - False positive rate: {1 - final_results['specificity']:.4f}
5. Business Threshold Results (Precision >= {min_precision}):
 - Threshold: {business_threshold:.3f}
 - Precision: {business_results['precision']:.4f}
 - Recall: {business_results['recall']:.4f}
 - F1 score: {business_results['f1']:.4f}
 - False positive rate: {1 - business_results['specificity']:.4f}
6. Most Important Features:
 - {feature_importance_df['Feature'].iloc[0]} ({feature_importance_df['Importance'].iloc[0]:.4f})
 - {feature_importance_df['Feature'].iloc[1]} ({feature_importance_df['Importance'].iloc[1]:.4f})
 - {feature_importance_df['Feature'].iloc[2]} ({feature_importance_df['Importance'].iloc[2]:.4f})
 - {feature_importance_df['Feature'].iloc[3]} ({feature_importance_df['Importance'].iloc[3]:.4f})
 - {feature_importance_df['Feature'].iloc[4]} ({feature_importance_df['Importance'].iloc[4]:.4f})
""")